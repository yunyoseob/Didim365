{"cells":[{"cell_type":"markdown","id":"836e759e-4f78-4336-b39b-e3f4bd0ce00e","metadata":{"id":"836e759e-4f78-4336-b39b-e3f4bd0ce00e"},"source":["# [SQL Generation in Text2SQL with TinyLlama's LLM Fine-tuning (analyticsvidhya.com)](https://www.analyticsvidhya.com/blog/2024/02/sql-generation-in-text2sql-with-tinyllamas-llm-fine-tuning/)"]},{"cell_type":"code","execution_count":null,"id":"5cbe2db5-9ed7-4c59-8e5e-9ec8e6f842f8","metadata":{"id":"5cbe2db5-9ed7-4c59-8e5e-9ec8e6f842f8","outputId":"a03f2842-a458-4e0a-9de5-903a0fc49299"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Apr 11 05:01:14 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100 80GB PCIe          On  | 00000000:00:05.0 Off |                    0 |\n","| N/A   40C    P0              45W / 300W |      4MiB / 81920MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"f64db02c-accc-45e7-bed1-a38e0d47e3c3","metadata":{"id":"f64db02c-accc-45e7-bed1-a38e0d47e3c3"},"source":["## Setting Up the Environment"]},{"cell_type":"code","execution_count":null,"id":"bdebce76-4dc9-4475-a16c-df5e1cf8da1c","metadata":{"id":"bdebce76-4dc9-4475-a16c-df5e1cf8da1c","outputId":"973e37a3-bbc7-41a8-d607-4eef569f8ba9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.61.tar.gz (37.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.4/37.4 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n","  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.24.1)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.61-cp310-cp310-manylinux_2_35_x86_64.whl size=39013369 sha256=702c1cfce710b14bc69e25aa1689ccf87ff267c238e08ef4a15a17f62d26a1b8\n","  Stored in directory: /root/.cache/pip/wheels/a4/d4/bc/483ff5c198ee745418fb35b59055591eeae2bc6ff69680537f\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, diskcache, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.4.0\n","    Uninstalling typing_extensions-4.4.0:\n","      Successfully uninstalled typing_extensions-4.4.0\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.61 typing-extensions-4.11.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.9.0)\n","Collecting fsspec>=2023.5.0 (from huggingface-hub)\n","  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n","Collecting tqdm>=4.42.1 (from huggingface-hub)\n","  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2022.12.7)\n","Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tqdm, fsspec, huggingface-hub\n","Successfully installed fsspec-2024.3.1 huggingface-hub-0.22.2 tqdm-4.66.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Collecting accelerate\n","  Downloading accelerate-0.29.2-py3-none-any.whl.metadata (18 kB)\n","Collecting peft\n","  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n","Collecting transformers\n","  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.8.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n","Collecting safetensors>=0.3.1 (from accelerate)\n","  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n","Collecting regex!=2019.12.17 (from transformers)\n","  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.19,>=0.14 (from transformers)\n","  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting datasets (from trl)\n","  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.3-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2024.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (15.0.7)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n","  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n","Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n","  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting pyarrow>=12.0.0 (from datasets->trl)\n","  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Collecting pyarrow-hotfix (from datasets->trl)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->trl)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting pandas (from datasets->trl)\n","  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Collecting xxhash (from datasets->trl)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets->trl)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec>=2023.5.0 (from huggingface-hub->accelerate)\n","  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting aiohttp (from datasets->trl)\n","  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Collecting aiosignal>=1.1.2 (from aiohttp->datasets->trl)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.1.0)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets->trl)\n","  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->trl)\n","  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->trl)\n","  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n","Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets->trl)\n","  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n","Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n","Collecting pytz>=2020.1 (from pandas->datasets->trl)\n","  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets->trl)\n","  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n","  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n","Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.8.1-py3-none-any.whl (225 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.8.3-py3-none-any.whl (102 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n","Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: pytz, xxhash, tzdata, shtab, safetensors, regex, pyarrow-hotfix, pyarrow, multidict, mdurl, fsspec, frozenlist, docstring-parser, dill, async-timeout, yarl, pandas, multiprocess, markdown-it-py, aiosignal, tokenizers, rich, aiohttp, tyro, transformers, datasets, accelerate, trl, peft, bitsandbytes\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.3.1\n","    Uninstalling fsspec-2024.3.1:\n","      Successfully uninstalled fsspec-2024.3.1\n","Successfully installed accelerate-0.29.2 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.43.0 datasets-2.18.0 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 fsspec-2024.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 peft-0.10.0 pyarrow-15.0.2 pyarrow-hotfix-0.6 pytz-2024.1 regex-2023.12.25 rich-13.7.1 safetensors-0.4.2 shtab-1.7.1 tokenizers-0.15.2 transformers-4.39.3 trl-0.8.1 tyro-0.8.3 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install llama-cpp-python\n","!pip3 install huggingface-hub\n","!pip3 install accelerate peft bitsandbytes transformers trl"]},{"cell_type":"markdown","id":"42944131-28bb-462e-b00e-db87a2bf70db","metadata":{"id":"42944131-28bb-462e-b00e-db87a2bf70db"},"source":["- CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\": llama-cpp-python을 빌드하는 동안 CUBLAS 라이브러리를 사용하여 GPU 가속을 활성화합니다.\n","- FORCE_CMAKE=1: cmake를 강제로 실행하여 새로운 빌드를 보장합니다.\n","- pip3 install llama-cpp-python: 양자화된 모델과 상호작용하려면  라이브러리가 필요합니다.\n","- !pip3 install accelerate peft bitsandbytes transformers trl:\n","```\n","accelerate: 훈련 프로세스를 여러 GPU 또는 machine에 분산시켜 훈련 시간을 크게 단축시킬 수 있음\n","\n","peft: 사용자 정의 데이터 세트에서 대규모 언어 모델을 미세 조정하기 위한 도구 및 기술 제공\n","\n","Bitsandbytes: LLM의 메모리 사용량을 줄여 메모리 리소스가 제한된 시스템에서 교육할 수 있도록 도와줌\n","\n","trl: 의사 결정 및 계획이 필요한 작업을 위해 LMM을 미세 조정하는 데 사용할 수 있는 강화 학습용 알고리즘 및 도구 제공\n","```"]},{"cell_type":"markdown","id":"02568b5c-226b-4188-a96e-013066aab915","metadata":{"id":"02568b5c-226b-4188-a96e-013066aab915"},"source":["## Modeling"]},{"cell_type":"markdown","id":"f2d4ae55-cc79-441b-8ac8-0312bbfc6ec1","metadata":{"id":"f2d4ae55-cc79-441b-8ac8-0312bbfc6ec1"},"source":["**model downlaod**"]},{"cell_type":"code","execution_count":null,"id":"4a299d99-7792-41f0-bf0e-5aebe3e08580","metadata":{"colab":{"referenced_widgets":["716e4b3e2f784d48b4280ad5e1965de7"]},"id":"4a299d99-7792-41f0-bf0e-5aebe3e08580","outputId":"7dba9f8b-fffd-4e7e-f43c-7d52704f2f45"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"716e4b3e2f784d48b4280ad5e1965de7","version_major":2,"version_minor":0},"text/plain":["tinyllama-1.1b-chat-v1.0.Q8_0.gguf:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model downloaded to: /root/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf\n"]}],"source":["from huggingface_hub import hf_hub_download\n","\n","# model_name: HuggingFace 모델 허브의 모델 이름\n","model_name = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n","\n","# Define the name of the model file to download.\n","model_file = \"tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n","\n","# Download the model from the Hugging Face Hub and store the\n","# path to the downloaded file in the `model_path` variable.\n","# model_path: HuggingFace에서 모델을 다운로드 한 후 모델 경로가 저장될 변수\n","model_path = hf_hub_download(model_name, filename=model_file)\n","\n","# Print a message indicating that the model has been downloaded.\n","print(f\"Model downloaded to: {model_path}\")"]},{"cell_type":"markdown","id":"28ebfaa2-bd78-4a50-81c8-6a2e297ccd26","metadata":{"id":"28ebfaa2-bd78-4a50-81c8-6a2e297ccd26"},"source":["**Initializing the Model**"]},{"cell_type":"code","execution_count":null,"id":"86e70178-ecea-4b35-95f1-46441aac20d3","metadata":{"id":"86e70178-ecea-4b35-95f1-46441aac20d3"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"id":"64e6c9f5-87a7-40a2-8c9d-ebc9bb094833","metadata":{"id":"64e6c9f5-87a7-40a2-8c9d-ebc9bb094833","outputId":"991622d7-0c50-456c-f711-57339f128fe1"},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /root/.cache/huggingface/hub/models--TheBloke--TinyLlama-1.1B-Chat-v1.0-GGUF/snapshots/52e7645ba7c309695bec7ac98f4f005b139cf465/tinyllama-1.1b-chat-v1.0.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 7\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n","llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   45 tensors\n","llama_model_loader: - type q8_0:  156 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 2048\n","llm_load_print_meta: n_embd           = 2048\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 4\n","llm_load_print_meta: n_layer          = 22\n","llm_load_print_meta: n_rot            = 64\n","llm_load_print_meta: n_embd_head_k    = 64\n","llm_load_print_meta: n_embd_head_v    = 64\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 256\n","llm_load_print_meta: n_embd_v_gqa     = 256\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 5632\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 2048\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 1B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 1.10 B\n","llm_load_print_meta: model size       = 1.09 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: PAD token        = 2 '</s>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.15 MiB\n","llm_load_tensors: offloading 22 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 23/23 layers to GPU\n","llm_load_tensors:        CPU buffer size =    66.41 MiB\n","llm_load_tensors:      CUDA0 buffer size =  1048.51 MiB\n","..........................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =    11.00 MiB\n","llama_new_context_with_model: KV self size  =   11.00 MiB, K (f16):    5.50 MiB, V (f16):    5.50 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =    66.50 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n","llama_new_context_with_model: graph nodes  = 710\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n","Using gguf chat template: {% for message in messages %}\n","{% if message['role'] == 'user' %}\n","{{ '<|user|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'system' %}\n","{{ '<|system|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'assistant' %}\n","{{ '<|assistant|>\n","'  + message['content'] + eos_token }}\n","{% endif %}\n","{% if loop.last and add_generation_prompt %}\n","{{ '<|assistant|>' }}\n","{% endif %}\n","{% endfor %}\n","Using chat eos_token: </s>\n","Using chat bos_token: <s>\n"]},{"name":"stdout","output_type":"stream","text":["Llama object initialized successfully.\n"]}],"source":["from llama_cpp import Llama\n","\n","# Initialize a `Llama` object with the downloaded model path.\n","# Llama: Class from llama_cpp library, that is worked with to initialize the model.\n","llm = Llama(\n","    # model_path: It is the path to the downloaded model that we obtained a while ago\n","    model_path=model_path,\n","\n","    # Set the number of context tokens.\n","    # n_ctx: This variable takes in the number of context tokens the model can handle. Here we are passing it a value of 512\n","    n_ctx=512,\n","\n","    # Set the number of threads to use.\n","    # n_threads: This variable takes in the number of CPU threads for computation. The Google Colab has a 4-core CPU, hence passing it 8 threads\n","    n_threads=8,\n","\n","    # Set the number of GPU layers to work with.\n","    # n_gpu_layers: This variable takes in the number of GPU layers in which the model needs to be offloaded. The value of 40 will offload the entire TinyLlama 1.1B within the Google Colab T4 GPU\n","    n_gpu_layers=40\n",")\n","\n","# Print a message indicating that the Llama object has been initialized.\n","print(\"Llama object initialized successfully.\")"]},{"cell_type":"code","execution_count":null,"id":"7193e3fe-49fb-474a-889e-b95784622ba0","metadata":{"id":"7193e3fe-49fb-474a-889e-b95784622ba0","outputId":"71742988-f250-4160-99a2-3e5e9c99ae4c"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    8652.62 ms\n","llama_print_timings:      sample time =      47.58 ms /   297 runs   (    0.16 ms per token,  6242.51 tokens per second)\n","llama_print_timings: prompt eval time =    8652.48 ms /    33 tokens (  262.20 ms per token,     3.81 tokens per second)\n","llama_print_timings:        eval time =    1259.11 ms /   296 runs   (    4.25 ms per token,   235.09 tokens per second)\n","llama_print_timings:       total time =   10531.77 ms /   329 tokens\n"]},{"name":"stdout","output_type":"stream","text":["I am not a robot. However, I can provide some general information about robots. Robots are machines designed to perform specific tasks and functions without human intervention. They may be programmed to perform repetitive or routine tasks, such as assembly lines or manufacturing plants, or they may be autonomous and self-sufficient. The primary purpose of a robot is to perform a task in an efficient and cost-effective manner while minimizing risks or errors associated with human intervention. Some common types of robots include automated machinery, such as assembly lines and manufacturing equipment; mobile robotics (such as self-driving cars); humanoid robots (such as the humanoid robot that Apollo 11 astronauts used for the moon landing); and humanoid robotic systems (such as the AI system in Star Trek). The use of robots in industry and science has grown rapidly over the past few decades, with advancements in technology making them more efficient, cost-effective, and reliable. Some current examples include the use of industrial robots in factories, the development of autonomous vehicles for transportation, and the creation of human-robot hybrids for medical applications such as surgery and therapy. Overall, while robots are not a new concept, they have evolved significantly in terms of technology, design, and application over time.\n"]}],"source":["# Use the Llama object to generate an answer to the question.\n","output = llm(\n","    # Prompt\n","    \"<|im_start|>user\\nAre you a robot?<|im_end|>\\n<|im_start|>assistant\\n\",\n","\n","    # Set the maximum number of tokens to generate.\n","    max_tokens=512,\n","\n","    # Set the stop sequences to indicate the end of the generated text.\n","    stop=[\"</s>\"],\n",")\n","\n","# Print the generated text.\n","print(output['choices'][0]['text'])"]},{"cell_type":"markdown","id":"a1e4c58a-93dc-484a-aad4-5640fc75f663","metadata":{"id":"a1e4c58a-93dc-484a-aad4-5640fc75f663"},"source":["## Testing the Vanilla TinyLlama"]},{"cell_type":"markdown","id":"235fbaf0-b244-45b1-8e6c-c22023629127","metadata":{"id":"235fbaf0-b244-45b1-8e6c-c22023629127"},"source":["Context와 Question을 제공하면 SQL 쿼리를 만드는 함수"]},{"cell_type":"code","execution_count":null,"id":"2f7580eb-dc51-470e-b055-d46cecdaef57","metadata":{"id":"2f7580eb-dc51-470e-b055-d46cecdaef57"},"outputs":[],"source":["def chat_template(question, context):\n","    \"\"\"\n","    Creates a chat template for the Llama model.\n","\n","    Args:\n","        question: The question to be answered.\n","        context: The context information to be used for generating the answer.\n","\n","    Returns:\n","        A string containing the chat template.\n","    \"\"\"\n","\n","    template = f\"\"\"\\\n","    <|im_start|>user\n","    Given the context, generate an SQL query for the following question\n","    context:{context}\n","    question:{question}\n","    <|im_end|>\n","    <|im_start|>assistant\n","    \"\"\"\n","    # Remove any leading whitespace characters from each line in the template.\n","    template = \"\\n\".join([line.lstrip() for line in template.splitlines()])\n","    return template"]},{"cell_type":"markdown","id":"8cf40350-cdf1-4316-abae-99eafacd4aa4","metadata":{"id":"8cf40350-cdf1-4316-abae-99eafacd4aa4"},"source":["**input으로 들어가게 될 template**"]},{"cell_type":"code","execution_count":null,"id":"662a4193-e582-4b3a-98e3-26a1a5f15819","metadata":{"id":"662a4193-e582-4b3a-98e3-26a1a5f15819","outputId":"0779be03-7014-40e5-cbbc-6fee787702ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["<|im_start|>user\n","Given the context, generate an SQL query for the following question\n","context:CREATE TABLE head (age INTEGER)\n","question:How many heads of the departments are older than 56 ?\n","<|im_end|>\n","<|im_start|>assistant \n","\n"]}],"source":["question = \"How many heads of the departments are older than 56 ?\"\n","context = \"CREATE TABLE head (age INTEGER)\"\n","print(chat_template(question,context))"]},{"cell_type":"markdown","id":"4193672d-543a-4811-821d-89b032ec1173","metadata":{"id":"4193672d-543a-4811-821d-89b032ec1173"},"source":["**output 출력**"]},{"cell_type":"code","execution_count":null,"id":"2b04d8f8-79e3-4ea8-9551-f3f25e994e6e","metadata":{"id":"2b04d8f8-79e3-4ea8-9551-f3f25e994e6e","outputId":"85969543-d429-44b9-cac9-ef4fa7bf244d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =    8652.62 ms\n","llama_print_timings:      sample time =      11.12 ms /    72 runs   (    0.15 ms per token,  6475.98 tokens per second)\n","llama_print_timings: prompt eval time =       9.31 ms /    62 tokens (    0.15 ms per token,  6658.08 tokens per second)\n","llama_print_timings:        eval time =     299.74 ms /    71 runs   (    4.22 ms per token,   236.87 tokens per second)\n","llama_print_timings:       total time =     447.52 ms /   133 tokens\n"]},{"name":"stdout","output_type":"stream","text":["To find the number of heads of the Department in which age is greater than or equal to 56, we can use a query like this:\n","```sql\n","SELECT COUNT(*) FROM head WHERE age >= 56;\n","```\n","This will count all heads in the `head` table where `age` is greater than 56.\n"]}],"source":["# Use the Llama object to generate an answer to the question.\n","output = llm(\n","    chat_template(question, context),\n","\n","    # Set the maximum number of tokens to generate.\n","    max_tokens=512,\n","\n","    # Set the stop sequences to indicate the end of the generated text.\n","    stop=[\"</s>\"],\n",")\n","\n","\n","# Print the generated text.\n","print(output['choices'][0]['text'])"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}